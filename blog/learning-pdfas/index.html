<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">

<!-- <title>Learning Probabilistic Deterministic Automata</title> -->
<!-- name and description are set by Jekyll SEO -->
<meta name = "robots" content = "index, follow">
<meta name = "keywords" content = "marco favorito, phd, computer science, artificial intelligence, research, sapienza">

<meta property="og:type" content="website"> 

<link rel="stylesheet" href="/assets/main.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
<link rel="stylesheet" href="https://code.jquery.com/ui/1.12.1/themes/base/jquery-ui.css">

<link rel="canonical" href="https://marcofavorito.me/blog/learning-pdfas/">
<link rel="alternate" type="application/rss+xml" title="Marco Favorito" href="/feed.xml">



<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Learning Probabilistic Deterministic Automata | Marco Favorito</title>
<meta name="generator" content="Jekyll v4.3.2" />
<meta property="og:title" content="Learning Probabilistic Deterministic Automata" />
<meta name="author" content="Marco Favorito" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Implementation of the (Palmer &amp; Goldberg, 2005) PAC algorithm." />
<meta property="og:description" content="Implementation of the (Palmer &amp; Goldberg, 2005) PAC algorithm." />
<link rel="canonical" href="https://marcofavorito.me/blog/learning-pdfas" />
<meta property="og:url" content="https://marcofavorito.me/blog/learning-pdfas" />
<meta property="og:site_name" content="Marco Favorito" />
<meta property="og:image" content="https://marcofavorito.me/probabilistic-automaton.jpg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-10-09T00:00:00+02:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:image" content="https://marcofavorito.me/probabilistic-automaton.jpg" />
<meta property="twitter:title" content="Learning Probabilistic Deterministic Automata" />
<meta name="twitter:site" content="@marcofavorit" />
<meta name="twitter:creator" content="@Marco Favorito" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Marco Favorito","url":"https://marcofavorito.me"},"dateModified":"2020-10-09T00:00:00+02:00","datePublished":"2020-10-09T00:00:00+02:00","description":"Implementation of the (Palmer &amp; Goldberg, 2005) PAC algorithm.","headline":"Learning Probabilistic Deterministic Automata","image":"https://marcofavorito.me/probabilistic-automaton.jpg","mainEntityOfPage":{"@type":"WebPage","@id":"https://marcofavorito.me/blog/learning-pdfas"},"publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"https://marcofavorito.me/favicon.ico"},"name":"Marco Favorito"},"url":"https://marcofavorito.me/blog/learning-pdfas"}</script>
<!-- End Jekyll SEO tag -->


    
<!-- MathJax -->
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>
    
  </head>

  <d-front-matter>
    <script async type="text/json">{
      "title": "Learning Probabilistic Deterministic Automata",
      "description": "Implementation of the (Palmer & Goldberg, 2005) PAC algorithm.",

      "published": "2020-10-09",
      "authors": [
        
        {
          "author": "Marco Favorito",
          "authorURL": "https://marcofavorito.me",
          "affiliations": [
            
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
      
      

    }</script>
  </d-front-matter>

  <body class="page">

    <!-- Header -->

    <header class="site-header">

  <div class="wrapper">
<!--
    <a class="site-title" rel="author" href="/">Marco Favorito</a>
	--><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger">
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewbox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"></path>
            </svg>
          </span>
        </label>

        <div class="trigger">
            
            <a class="page-link " href="/">About</a>
            
            <a class="page-link " href="/papers">Publications</a>
            
        </div>
      </nav>
</div>
</header>



    <!-- Content -->
    <main class="page-content" aria-label="Content">
    <div class="post distill">

      <d-title>
        <h1>Learning Probabilistic Deterministic Automata</h1>
        <p>Implementation of the (Palmer &amp; Goldberg, 2005) PAC algorithm.</p>
      </d-title>

      <d-byline></d-byline>

      <d-article class="l-body">
        <p>These days I’ve been working on a problem that required the learning
of a Probabilistic Deterministic Automata (PDFA) <d-cite key="pdfa63"></d-cite>.
In particular, the learning algorithm we are using
provides guarantees on the total variation distance between
the ground truth automaton $\mathcal{A}$ and the learned automaton $\mathcal{A}^\prime$ <d-cite key="palmer2005"></d-cite>.
In the following, I will provide more background definitions and details on my implementation of the algorithm.</p>

<h2 id="pdfa">PDFA</h2>

<p>A <em>Probabilistic Deterministic Finite Automaton</em> <d-cite key="pdfa63"></d-cite> $\mathcal{A}$ is 
a tuple $\langle Q, \Sigma, q_0, q_f, \tau, \gamma \rangle$ where:</p>

<ul>
  <li>$Q$ is the set of states.</li>
  <li>$\Sigma$ is the alphabet.</li>
  <li>$q_0 \in Q$ is the initial state.</li>
  <li>$q_f \not\in Q$ is the final state.</li>
  <li>$\tau: Q \times \Sigma \to Q$ is the transition function.</li>
  <li>$\gamma: Q \times \Sigma \to [0, 1]$ is a function that associates a transition 
$(q, \sigma)$ to the probability that that transition can be taken.</li>
</ul>

<p>Other requirements for a valid PDFA are:</p>
<ul>
  <li>$\forall q.\sum_{\sigma \in \Sigma} \gamma(q, \sigma) = 1$, that is, the probabilities of the outgoing transitions from a given state $q$ must sum to $1$.</li>
  <li>when $\tau(q, \sigma)$ is undefined, $\gamma(q, \sigma) = 0$.</li>
  <li>$\forall q. \exists s\in\Sigma^*. \tau(q, s) = q_f \wedge \gamma(q, s) &gt; 0$, i.e. $q_f$ is reachable with probability non-zero from any state.</li>
</ul>

<p>A PDFA $\mathcal{A}$ defines a probability distribution over strings in $\Sigma^*$. Notice that 
the reachability of the final state $q_f$ ensures that any execution of $\mathcal{A}$ will halt with probability $1$.</p>

<p>It is useful to extend $\tau$ and $\gamma$ for succinteness purposes, i.e.:</p>
<ul>
  <li>$\tau(q, s) = \tau(\tau(q, \sigma_1), \sigma_2…\sigma_k$</li>
  <li>$\gamma(q, s) = \gamma(q, \sigma_1) \cdot \gamma(\tau(q, \sigma_1), \sigma_2…\sigma_k)$</li>
</ul>

<p>Let $D_{\mathcal{A}}(s)$ be the probability distribution over strings $\Sigma^*$.</p>

<p>We have:</p>

<p style="text-align: center;">
$D_{\mathcal{A}}(s) = \gamma(q_0,s)$ for $s$ such that $\tau(q_0,s) = q_f$.
</p>

<p>Moreover, we define the <em>variation distance</em> $L_1$ between two distributions
$D_1$ and $D_2$ over $\Sigma^<em>$ as 
$L_1(D_1, D_2) = \sum_{s\in\Sigma^</em>} | D_1(s) - D_2(s) |$.</p>

<p>Later, it will be useful the following definition of $L_{\infty}$-norm between two distributions $D$, $D^\prime$:</p>

<p style="text-align: center;">
$L_{\infty}(D, D^\prime) = \max_{s\in\Sigma^*} |D(s) - D^\prime(s)|$
</p>

<p><img src="simple-pdfa-example.jpeg" alt="A Simple PDFA with $\zeta,\zeta^\prime\in [0, 1]$ "></p>

<h2 id="pdfas-in-python">PDFAs in Python</h2>

<p>I implemented a <a href="https://github.com/marcofavorito/pdfa-learning">tiny Python library</a><d-footnote>Not meant to be production-ready <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20"></d-footnote> to handle PDFAs.
In the following code snippet, you can see the implementation of the class for representing
a PDFA.</p>

<d-code block="" language="python">
"""Base module of the PDFA package."""

from dataclasses import dataclass
from typing import AbstractSet, Collection, Set, Tuple

import numpy as np

from src.helpers.base import assert_
from src.pdfa.helpers import (
    _check_ergodicity,
    _check_is_legal_character,
    _check_is_legal_state,
    _check_is_legal_word,
    _check_transitions_are_legal,
)
from src.pdfa.types import Character, State, TransitionFunctionDict, Word


@dataclass(frozen=True)
class PDFA:
    """
    Probabilistic Deterministic Finite Automaton.

    - The set of states is the set of integers {0, ..., nb_states - 1} (nb_states &gt; 0)
    - The alphabet is the set of integers {0, ..., alphabet_size - 1}
    - The initial state is always 0
    - The final state is always "nb_states"
    - The transition function is a nested dictionary:
        - at the first level, we have states as keys and the dict of outgoing transition_dict as values
        - a dict of outgoing transition_dict has characters as keys and a tuple of next state and probability
          as value.

    At initialization times, checks on the consistency of the transition dictionary are done.
    """

    nb_states: int
    alphabet_size: int
    transition_dict: TransitionFunctionDict

    def __post_init__(self):
        """Post-initialization checks."""
        assert_(self.nb_states &gt; 0, "Number of states must be greater than zero.")
        assert_(self.alphabet_size &gt; 0, "Alphabet size must be greater than zero.")
        _check_transitions_are_legal(
            self.transition_dict, self.nb_states, self.alphabet_size
        )
        _check_ergodicity(self.transition_dict, self.nb_states, self.final_state)

    def get_successor(self, state: State, character: Character) -&gt; State:
        """
        Get the successor state.

        :param state: the starting state.
        :param character: the read symbol.
        :return: the new state.
        """
        _check_is_legal_state(state, self.nb_states)
        _check_is_legal_character(character, self.alphabet_size)
        next_transitions = self.transition_dict.get(state, {})
        assert_(
            character in next_transitions,
            f"Cannot read character {state} from state {character}.",
        )
        next_state, _probability = next_transitions[character]
        return next_state

    def get_successors(self, state: State) -&gt; AbstractSet[State]:
        """Get the successors."""
        _check_is_legal_state(state, self.nb_states)
        return {
            successor
            for _character, (successor, _probability) in self.transition_dict[
                state
            ].items()
        }

    def get_next_transitions(
        self, state: State
    ) -&gt; Collection[Tuple[Character, float, State]]:
        """Get next transitions from a state."""
        _check_is_legal_state(state, self.nb_states)
        return {
            (character, probability, successor)
            for character, (successor, probability) in self.transition_dict[
                state
            ].items()
        }

    @property
    def initial_state(self):
        """Get the initial state."""
        return 0

    @property
    def final_state(self) -&gt; State:
        """Get the final state."""
        return self.nb_states

    @property
    def states(self) -&gt; Set[State]:
        """Get the set of states."""
        return set(range(self.nb_states))

    @property
    def transitions(self) -&gt; Collection[Tuple[State, Character, float, State]]:
        """Get the transitions."""
        return {
            (start, char, prob, end)
            for start, out_transitions in self.transition_dict.items()
            for char, (end, prob) in out_transitions.items()
        }

    def get_probability(self, word: Word):
        """Get the probability of a word."""
        if len(word) == 0:
            return 0.0

        _check_is_legal_word(word, self.alphabet_size)
        result = 1.0
        current_state = self.initial_state
        for character in word:
            if current_state is None or current_state == self.final_state:
                result = 0.0
                break
            next_state, probability = self.transition_dict.get(current_state, {}).get(
                character, (None, 0.0)
            )
            current_state = next_state
            result *= probability
        return 0.0 if current_state != self.final_state else result

    def sample(self) -&gt; Word:
        """Sample a word."""
        current_state = self.initial_state
        word = []
        while current_state != self.final_state:
            characters, probabilities, next_states = zip(
                *self.get_next_transitions(current_state)
            )
            index = np.random.choice(range(len(characters)), p=probabilities)
            next_character = characters[index]
            current_state = next_states[index]
            word.append(next_character)
        return word

</d-code>

<p>In particular:</p>
<ul>
  <li>The set of states is the set of integers $\{ 0, …, nb\_states - 1\}$ (with $nb\_states &gt; 0$)</li>
  <li>The alphabet is the set of integers $\{0, …, alphabet\_size - 1\}$</li>
  <li>The initial state is always $q_0 = 0$</li>
  <li>The final state is always $q_f = nb\_states$</li>
  <li>The transition function is a nested dictionary:
    <ul>
      <li>at the first level, we have states as keys and the dict of outgoing $transition\_dict$ as values</li>
      <li>a dict of outgoing transitions has characters $\sigma$ as keys and a tuple of next state and probability
$(q^\prime, p)$ as value.</li>
    </ul>
  </li>
</ul>

<p>For example, to create an object representing the PDFA showed above (with $\zeta = 0.4$ and $\zeta^\prime = 0.7$):</p>

<d-code block="" language="python">
from src.pdfa import PDFA
p1 = 0.4
p2 = 0.7
automaton = PDFA(
    nb_states=2,
    alphabet_size=2,
    transition_dict={
        0: {
            0: (1, p1),
            1: (2, 1 - p1),
        },
        1: {
            0: (2, 1 - p2),
            1: (1, p2),
        },
    },
)
</d-code>

<p>There is a helper function to render the automaton with Graphviz:</p>

<d-code block="" language="python">
from src.pdfa.render import to_graphviz
to_graphviz(automaton).render("path-to-file")
</d-code>

<p>That results in:</p>

<p><img src="./rendered-automaton.svg" alt="Result."></p>

<h2 id="learning-a-pdfa">Learning a PDFA</h2>

<p>As stated earlier, the library also implements a PDFA learning algorithm, proposed in <d-cite key="palmer2005"></d-cite>.</p>

<p>There exist other approaches (<d-cite key="ron1998,clark2004"></d-cite>). In <d-cite key="ron1998"></d-cite>, they show how to learn <em>acyclic</em> PDFAs, 
and apply the algorithm to speech and handwriting recognition. In <d-cite key="clark2004"></d-cite>, 
they are able to learn general PDFAs, using an upper-bound in terms of the Kullback–Leibler divergence <d-cite key="kullback1951"></d-cite> 
between the true distribution and the learned one. The one we are going to describe learns a PDFA
whose total variation distance from the true PDFA is upper-bounded by an $\varepsilon$:</p>

<p style="text-align: center;">
$L(D, \hat{D}) = \sum_{s\in\Sigma^*} |D(s) - \hat{D}(s)| \le \varepsilon$
</p>

<p>All the mentioned algorithms are <em>Probably Approximately Correct</em> for some probability $\delta$ to fail and some error
tolerance $\varepsilon$ <d-cite key="valiant1984"></d-cite>. That is, with probability $1-\delta$, 
they learn a model whose error measure is guaranteed to be upper-bounded by some quantity depending on $\varepsilon$.</p>

<p>An important concept in the context of the learning of a PDFA is the <em>distinguishability</em> of
any pair of states. Formally, we say that a pair of nodes
$(q_1,q_2)$ are <em>$\mu$-distinguishable</em> if</p>

<p style="text-align: center;">
$L_{\infty}(D_{q_1}, D_{q_2}) = \max_{s\in\Sigma^*} |D_{q_1}(s) - D_{q_2}(s)| \ge \mu$
</p>
<p>where $D_{q}$ is the distribution represented by the automaton starting from $q$.
The idea is that $q_1$ and $q_2$ should have sufficiently different suffix distributions 
in order to be considered separate states.</p>

<p>The algorithm is quite complex to be explained in detail here.
It takes in input the following parameters:</p>
<ul>
  <li>$\Sigma$: the alphabet size.</li>
  <li>$n$: an upper bound on the number of states of the target automaton.</li>
  <li>$\mu$: a lower bound on distinguishability.</li>
</ul>

<p>Plus the PAC parameters $\delta$ and $\varepsilon$.</p>

<p>It is composed in two parts:</p>
<ul>
  <li>Estimation of a <em>subgraph</em> $H$ of the underlying graph of a PDFA;</li>
  <li>Estimation of the probabilities on the edges.</li>
</ul>

<p>Here the pseudocode of both algorithms:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="algorithm-1.png" alt=""></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Find a subgraph $H$ of $\mathcal{A}$</td>
    </tr>
  </tbody>
</table>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="algorithm-2.png" alt=""></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Find the probabilities $\gamma(q,\sigma)$ forall $q,\sigma$</td>
    </tr>
  </tbody>
</table>

<p>For both algorithms, we need to generate a certain amount of samples from the true PDFA $\mathcal{A}$.
However, despite the number of samples is polynomial 
in terms of all the parameters of the algorithm,
the amount of samples needed for the PAC guarantees is <em>computationally prohibitive</em> 
for even reasonable assignment of such parameters.</p>

<p>For example, say we want to learn an automaton with $n=2$, $|\Sigma| = 2$, $\mu = 0.1$, and 
PAC parameters $\varepsilon = 0.1$ and $\delta^\prime = \delta^{\prime\prime} = 0.1$ 
($\delta^\prime$ is the probability of failure of Algorithm 1, whereas 
 $\delta^{\prime\prime}$ is the probability of failure of Algorithm 2. They are
 such that $\delta^\prime + \delta^{\prime\prime} = \delta$).</p>

<p>The following code snippet compute the number of samples for Algorithm 1:</p>

<d-code block="" language="python">
"""Compute sample size for Algorithm 1."""
from math import ceil, log2, log

def compute_m0(n: int, s: int, mu: float, eps: float, delta_1: float):
    return ceil((16 / mu) ** 2 * (log2(16 / delta_1 / mu) + log2(n * s) + n * s))

def sample_size_1(n: int, s: int, mu: float, eps: float, delta_1: float):
    """Compute N for Algorithm 1."""
    m0 = compute_m0(n, s, mu, eps, delta_1)
    N1 = 8 * (n ** 2) * (s ** 2) / (eps ** 2) * (log((2 ** (n * s)) * n * s / delta_1))
    N2 = 4 * m0 * n * s / eps
    N = ceil(max(N1, N2))
    return N

</d-code>

<p>This one does the same but for Algorithm 2:</p>
<d-code block="" language="python">
"""Compute sample size for Algorithm 2."""
from math import ceil, log

def sample_size_2(n: int, s: int, mu: float, eps: float, delta_2: float):
    n1 = 2 * n * s / delta_2
    n2 = (64 * n * s / eps / delta_2) ** 2
    n3 = 32 * n * s / eps
    n4 = log(2 * n * s / delta_2)
    N = ceil(n1 * n2 * n3 * n4)
    return N

</d-code>

<p>The number of samples for Algorithm 1 is given by:</p>

<d-code block="" language="python">
n = 2
s = 2
mu = 0.1
eps = 0.1
delta_1 = 0.1
N = sample_size_1(n, s, mu, eps, delta_1)
</d-code>

<p>Where <code class="language-plaintext highlighter-rouge">N</code> is <code class="language-plaintext highlighter-rouge">68173280</code> ($\approx 68 \cdot 10^6$ number of samples).</p>

<p>For Algorithm 2:</p>
<d-code block="" language="python">
delta_2 = 0.1
N = sample_size_2(n, s, mu, eps, delta_2)
</d-code>

<p>The result in <code class="language-plaintext highlighter-rouge">N</code> is <code class="language-plaintext highlighter-rouge">294072829470708</code> ($\approx 3 \cdot 10^{15}$ number of samples).</p>

<p>As said, these numbers are not practical for any personal computer. 
However, for some examples, a much lower number of samples can be enough,
as shown in the next section.</p>

<h2 id="implementation-of-the-learning-algorithm">Implementation of the learning algorithm</h2>

<p>In the <a href="https://github.com/marcofavorito/pdfa-learning">same repository</a>
(in <a href="https://github.com/marcofavorito/pdfa-learning/tree/main/src/learn_pdfa">src/learning-pdfas/</a>),
you can find the implementation of the <a href="https://www.sciencedirect.com/science/article/pii/S0304397507005476">(Puterman &amp; Goldberg, 2005) algorithm</a>.</p>

<p>You should execute the code snippet in the previous sections
in order to get the <code class="language-plaintext highlighter-rouge">automaton</code> variable.</p>

<d-code block="" language="python">
from src.learn_pdfa.base import learn_pdfa
from src.learn_pdfa.common import Generator, MultiprocessedGenerator, SimpleGenerator

generator = MultiprocessedGenerator(SimpleGenerator(automaton), nb_processes=8)

learned_pdfa = learn_pdfa(
    sample_generator=generator,
    alphabet_size=2,
    epsilon=0.4,
    delta_1=0.2,
    delta_2=0.2,
    mu=0.1,
    n=3,
    # the following are upper bound to the number of samples.
    n1_max_debug=3000000,
    n2_max_debug=1000000,
    m0_max_debug=3000000 / 10,
)
</d-code>

<p>The <code class="language-plaintext highlighter-rouge">generator</code> is an object that generates the samples from a <code class="language-plaintext highlighter-rouge">PDFA</code> instance.
<code class="language-plaintext highlighter-rouge">SimpleGenerator</code> is just a wrapper to an automaton, whereas 
<code class="language-plaintext highlighter-rouge">MultiprocessedGenerator</code> parallelizes the sampling using multiprocessing. <d-footnote>Would be interesting to devise a sampling strategy that is _not_ sequential.</d-footnote></p>

<p>Notice the parameters <code class="language-plaintext highlighter-rouge">n1_max_debug1</code>, <code class="language-plaintext highlighter-rouge">n2_max_debug</code> and <code class="language-plaintext highlighter-rouge">m0_max_debug</code>.
They are used to cap the number of samples for Algorithm 1, 
the number of samples for Algorithm 2, and
the value $m_0$, respectively.</p>

<p>You can see the same example <a href="https://github.com/marcofavorito/pdfa-learning/blob/main/notebooks/02-pdfa-learning.ipynb">in this notebook</a>.</p>

<h2 id="conclusion">Conclusion</h2>

<p>This library provides a simple (Python) implementation of Probabilistic Deterministic Finite Automata,
and a PAC-learning algorithm <d-cite key="palmer2005"></d-cite>.
As future work,  the other algorithms could be implemented, e.g. the one for the acyclic PDFAs <d-cite key="ron1998"></d-cite> 
and the one using the KL divergence as error metric <d-cite key="clark2004"></d-cite>.</p>

<h2 id="references">References</h2>


      
      <hr>
      <!-- disqus comments -->
      
        <div id="disqus_thread"></div>
<script>

/**
 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
var disqus_config = function () {
this.page.url = document.location.href; // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = document.location.pathname; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://marcofavorito-me.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
</noscript>               

      

      </d-article>


      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

      <d-bibliography src="./learning-pdfas.bib">
      </d-bibliography>
    </div>
  </main>

    <!-- Footer -->

    <footer class="site-footer">

  <div class="wrapper">
  
    
      <span class="footer-icon">
         <a href="mailto:marco.favorito@gmail.com"><img width="24" height="24" src="/assets/img/icons/envelope.svg"></a>
      </span>
  
    
      <span class="footer-icon">
         <a href="mailto:favorito@diag.uniroma1.it"><img width="24" height="24" src="/assets/img/icons/envelope.svg"></a>
      </span>
  
    
      <span class="footer-icon">
         <a href="https://t.me/marcofavorito"><img width="24" height="24" src="/assets/img/icons/telegram.svg"></a>
      </span>
  
    
      <span class="footer-icon">
         <a href="https://discord.com/users/marcofavorito#4943"><img width="24" height="24" src="/assets/img/icons/discord.png"></a>
      </span>
  
    
      <span class="footer-icon">
         <a href="https://github.com/marcofavorito"><img width="24" height="24" src="/assets/img/icons/github.svg"></a>
      </span>
  
    
      <span class="footer-icon">
         <a href="https://www.linkedin.com/in/marcofavorito/"><img width="24" height="24" src="/assets/img/icons/linkedin.svg"></a>
      </span>
  
    
      <span class="footer-icon">
         <a href="https://keybase.io/marcofavorito/"><img width="24" height="24" src="/assets/img/icons/keybase.svg"></a>
      </span>
  
    
      <span class="footer-icon">
         <a href="https://www.reddit.com/user/marcofavorito"><img width="24" height="24" src="/assets/img/icons/reddit.svg"></a>
      </span>
  
    
      <span class="footer-icon">
         <a href="https://news.ycombinator.com/user?id=marcofavorito"><img width="24" height="24" src="/assets/img/icons/hn.png"></a>
      </span>
  
    
      <span class="footer-icon">
         <a href="/assets/public_pgp.txt"><img width="24" height="24" src="/assets/img/icons/pgp.png"></a>
      </span>
  
    
      <span class="footer-icon">
         <a href="https://scholar.google.it/citations?user=tJhhDGEAAAAJ"><img width="24" height="24" src="/assets/img/icons/scholar.svg"></a>
      </span>
  
    
      <span class="footer-icon">
         <a href="https://dblp.uni-trier.de/pers/hd/f/Favorito:Marco"><img width="24" height="24" src="/assets/img/icons/dblp.png"></a>
      </span>
  
    
      <span class="footer-icon">
         <a href="bitcoin:BC1QSNTNEWWS350D6GT25WC4SXNZFEY55WSE7V0RGJ"><img width="24" height="24" src="/assets/img/icons/bitcoin.png"></a>
      </span>
  
    
      <span class="footer-icon">
         <a href=""><img width="24" height="24" src="/assets/img/icons/zcash.png"></a>
      </span>
  
    
        
    
      <span class="footer-icon">
         <a href="https://www.researchgate.net/profile/Marco-Favorito"><img width="24" height="24" src="/assets/img/icons/researchgate.png"></a>
      </span>
   

<div style="display: inline; text-align: center" itemscope itemtype="https://schema.org/Person">
  <a itemprop="sameAs" content="https://orcid.org/0000-0001-9566-3576" href="https://orcid.org/0000-0001-9566-3576" target="orcid.widget" rel="me noopener noreferrer" style="vertical-align:top;">
      <img src="/assets/img/icons/orcid_bw.png" width="24" height="24" alt="ORCID iD icon"></a>
</div>
      <!-- https://orcid.org/0000-0001-9566-3576 -->

</div>

</footer>


  </body>

  <!-- jQuery -->
<script src="https://code.jquery.com/jquery-3.5.1.min.js" integrity="sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js" integrity="sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.min.js" integrity="sha384-+YQ4JLhjyBLPDQt//I+STsc9iw4uQqACwlvpslubQzn4u2UU2UFM80nGisd026JF" crossorigin="anonymous"></script>
<!--<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap//js/mdb.min.js" integrity="" crossorigin="anonymous"></script> -->


  

  
<!-- Global site tag (gtag.js) - Google Analytics -->
<script src="/assets/js/publications.js"></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-162936928-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() { dataLayer.push(arguments); }
  gtag('js', new Date());

  gtag('config', 'UA-162936928-1');
  
  gtag('config', 'G-VLMZ2VWC3N');
  
  gtag('config', 'G-DDYB9GX4XD');
  
  gtag('set', 'anonymizeIp', true);
  gtag('send', 'pageview');
</script>




</html>